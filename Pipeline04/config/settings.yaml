# Configurações de caminhos e diretórios
paths:
  raw_data_dir: "data/raw/"
  processed_data_dir: "data/processed/"
  validated_data_dir: "data/validated/"
  output_data_dir: "data/output/"
  log_file: "logs/pipeline.log"

# Ingestão
ingestion:
  file_extensions:
    - ".log"
    - ".txt"
  recursive: true

#Estrutura esperada dos logs
log_schema:
  required_fields:
    - "timestamp"
    - "level"
    - "service"
    - "message"
    - "user_id"
    - "host"
    - "data_particao"

# Validação
validation:
  discard_if_missing:
    - "timestamp"
    - "level"

# Transformações
transformation:
  partition_by: ["year", "month", "day"]  # Extraído do timestamp

# Load
load:
  s3_bucket_name: "seu-bucket"
  s3_output_path: "logs/parquet/"
  use_s3: true  #se for false, salva apenas localmente

# Airflow / Execução
execution:
  airflow_enabled: true
  schedule_interval: "@daily"
